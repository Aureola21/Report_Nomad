\documentclass{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}

\title[NoMaD]{NoMaD: \textbf{N}avigati\textbf{o}n with Goal-\textbf{Ma}sked \textbf{D}iffusion}
\author{Sehaj Ganjoo, Shobhnik Kriplani, \\ Abhishek Kumar Jha, Namashivayaa V}
\institute{IISc Bengaluru \newline BTech. Mathematics and Computing}
\date{April 2025}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Motivation and Goal}
    \textbf{Robotic navigation in unfamiliar environments requires:}
    \begin{itemize}
        \item Task-oriented navigation — reaching specified goals
        \item Task-agnostic exploration — discovering and mapping new areas
    \end{itemize}
    
    \begin{block}{The Challenge}
        These two objectives are typically handled by \textit{separate systems}.\\[1ex]
        Exploration can be decomposed into:
        \begin{itemize}
            \item \textbf{Local Exploration:} Learning short-horizon control policies for diverse actions
            \item \textbf{Global Planning:} Using those policies to achieve long-horizon, goal-directed behavior
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Question}
        Can a \textit{single model} unify both tasks — exploration and navigation?
    \end{block}
    \end{frame}
    \begin{frame}{What is NoMaD?}
        \textbf{NoMaD} is a transformer-based diffusion policy designed for long-horizon, memory-efficient navigation.\\[0.5em]
        It supports both:
        \begin{itemize}
            \item \textbf{Goal-conditioned navigation} — moving towards a specified visual goal
            \item \textbf{Open-ended exploration} — learning diverse behaviors without explicit goals
        \end{itemize}
        
        \bigskip
        \texttt{NoMaD = \{EfficientNet + Vision Transformer\} $\leftarrow$ \textbf{ViNT} \\+ Diffusion Policies}
        
        \bigskip
        \begin{block}{}
            It combines a transformer backbone to encode the high-dimensional visual stream, with diffusion models that predict a sequence of future actions in a generative manner.
        \end{block}
        \end{frame}

\begin{frame}{Overview of NoMaD Architecture}
    \begin{block}{Visual Goal-Conditioned Navigation}
        Backbone: ViNT (Visual Navigation Transformer)\\
        How does ViNT work?
        \begin{itemize}
            \item Recieves: A sequence of past and current observations $o_t$=$o_{t-P:t}$
            \item \textbf{Visual Encoder:} Each observation is processed using an EfficientNet-B0 encoder to extract feature embeddings.
        \end{itemize}
        \pause
        \texttt{EfficientNet?}
        \begin{itemize}
            \item A new method of Scaling CNNs to improve accuracy and efficiency
            \item It uses a compound scaling method to uniformly scale all dimensions of depth, width, and resolution.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Architecture Overview}
\begin{itemize}
    \item \textbf{Perception:} EfficientNet-B0 backbone $\rightarrow$ Transformer-based temporal encoder.
    \item \textbf{Diffusion Decoder:} Conditional UNet1D for generating waypoint sequences.
    \item \textbf{Action Decoder:} Maps waypoints to low-level actions.
\end{itemize}
% \includegraphics[width=0.8\linewidth]{architecture.png}
\end{frame}

\begin{frame}{Training Procedure}
\begin{itemize}
    \item Dataset: SACSoN / RECON / GoStanford
    \item Batch size: 32, Epochs: 100
    \item Optimizer: AdamW, LR: $10^{-4}$
    \item Scheduler: Cosine annealing
    \item Loss: MSE on predicted noise + temporal distance
\end{itemize}
\[ \mathcal{L}_{NoMaD} = MSE(\epsilon, \hat{\epsilon}) + \lambda \cdot MSE(d(o_t, o_g), f_d(c_t)) \]
\end{frame}

\begin{frame}{Experiments and Results}
\textbf{Metrics:}
\begin{itemize}
    \item Diffusion Loss $\approx 1.11$
    \item Distance Loss $\approx 128$
    \item Cosine Similarity $\approx 0.47$
\end{itemize}
\textbf{Comparison with ViNT:}
\begin{itemize}
    \item Similar performance in goal-conditioned tasks
    \item No performance degradation when adding diffusion
\end{itemize}
\end{frame}

\begin{frame}{Challenges Faced}
\begin{itemize}
    \item CUDA Out Of Memory errors on limited GPU
    \item Module import issues with nested folder structures
    \item Gradients not propagating due to detached variables
\end{itemize}
\end{frame}

\begin{frame}{Team Contributions}
\textbf{Sehaj Ganjoo:}
\begin{itemize}
    \item Set up training pipeline and environment
    \item Integrated and debugged diffusion model
    \item Wrote training script and logging tools
    \item Conducted experiments and generated plots
    \item Created report and presentation
\end{itemize}
\end{frame}

\begin{frame}{Conclusion and Future Work}
\begin{itemize}
    \item Successfully trained NOMAD using diffusion for visual navigation
    \item Showed compatibility with ViNT-based perception
    \item Future work:
    \begin{itemize}
        \item Evaluate in simulation / real-world
        \item Improve runtime performance
        \item Try larger ViTs and alternate decoders
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Q\&A}
Thank you! \newline
\textit{Questions are welcome.}
\end{frame}

\end{document}